{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Worksheet 8 — Decision Tree, Ensemble Methods, and Hyperparameter Tuning\n",
        "\n",
        "## 1. Import Required Libraries\n",
        "\n",
        "In this section, we import all necessary Python libraries for:\n",
        "- Data handling\n",
        "- Machine learning models\n",
        "- Evaluation metrics\n",
        "- Hyperparameter tuning\n",
        "- Visualization"
      ],
      "metadata": {
        "id": "XFWsQE7b1K6r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "MDdhEfFlyzRj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "\n",
        "from sklearn.datasets import load_iris, load_wine, load_diabetes\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.metrics import accuracy_score, f1_score, mean_squared_error"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Helper Functions for Evaluation\n",
        "\n",
        "\n",
        "The following helper functions are used to evaluate:\n",
        "- **Classification models** using Accuracy and F1-score\n",
        "- **Regression models** using MSE and RMSE"
      ],
      "metadata": {
        "id": "4A3pSJgb1fCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def report_classification_results(y_true, y_pred, label=None):\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    f1_macro = f1_score(y_true, y_pred, average='macro')\n",
        "    print(f\"{label} -> Accuracy: {acc:.4f}, F1 (macro): {f1_macro:.4f}\")\n",
        "    return acc, f1_macro\n",
        "\n",
        "\n",
        "def report_regression_results(y_true, y_pred, label=None):\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    print(f\"{label} -> RMSE: {rmse:.4f}, MSE: {mse:.4f}\")\n",
        "    return mse, rmse"
      ],
      "metadata": {
        "id": "u2ZBL1F12Ylb"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Custom Decision Tree (Classification)\n",
        "\n",
        "\n",
        "This is a **from-scratch implementation** of a Decision Tree classifier using:\n",
        "- Entropy\n",
        "- Information Gain\n",
        "\n",
        "\n",
        "This implementation is **educational**, not optimized for performance."
      ],
      "metadata": {
        "id": "b0-8HeUZ1lfc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDecisionTree:\n",
        "    def __init__(self, max_depth=None, min_samples_split=2):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.tree = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.tree = self._build_tree(np.array(X), np.array(y), depth=0)\n",
        "\n",
        "    def _entropy(self, y):\n",
        "        counts = np.bincount(y)\n",
        "        probs = counts / counts.sum()\n",
        "        probs = probs[probs > 0]\n",
        "        return -np.sum(probs * np.log2(probs))\n",
        "\n",
        "    def _information_gain(self, y, y_left, y_right):\n",
        "        w_left = len(y_left) / len(y)\n",
        "        w_right = len(y_right) / len(y)\n",
        "        return self._entropy(y) - (w_left * self._entropy(y_left) + w_right * self._entropy(y_right))\n",
        "\n",
        "    def _best_split(self, X, y):\n",
        "        best_gain, split_idx, split_thr = -1, None, None\n",
        "        for idx in range(X.shape[1]):\n",
        "            thresholds = np.unique(X[:, idx])\n",
        "            for thr in thresholds:\n",
        "                left = y[X[:, idx] <= thr]\n",
        "                right = y[X[:, idx] > thr]\n",
        "                if len(left) == 0 or len(right) == 0:\n",
        "                    continue\n",
        "                gain = self._information_gain(y, left, right)\n",
        "                if gain > best_gain:\n",
        "                    best_gain, split_idx, split_thr = gain, idx, thr\n",
        "        return split_idx, split_thr\n",
        "\n",
        "    def _build_tree(self, X, y, depth):\n",
        "        if len(np.unique(y)) == 1:\n",
        "            return y[0]\n",
        "        if self.max_depth is not None and depth >= self.max_depth:\n",
        "            return np.bincount(y).argmax()\n",
        "\n",
        "        idx, thr = self._best_split(X, y)\n",
        "        if idx is None:\n",
        "            return np.bincount(y).argmax()\n",
        "\n",
        "        left_mask = X[:, idx] <= thr\n",
        "        right_mask = X[:, idx] > thr\n",
        "        return {\n",
        "            'feature': idx,\n",
        "            'threshold': thr,\n",
        "            'left': self._build_tree(X[left_mask], y[left_mask], depth + 1),\n",
        "            'right': self._build_tree(X[right_mask], y[right_mask], depth + 1)\n",
        "        }\n",
        "\n",
        "    def _predict(self, x, node):\n",
        "        if not isinstance(node, dict):\n",
        "            return node\n",
        "        if x[node['feature']] <= node['threshold']:\n",
        "            return self._predict(x, node['left'])\n",
        "        return self._predict(x, node['right'])\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.array([self._predict(x, self.tree) for x in X])"
      ],
      "metadata": {
        "id": "lNFQ261w1qQd"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Custom vs scikit-learn Decision Tree (Iris Dataset)\n",
        "\n",
        "We compare:\n",
        "- Custom Decision Tree\n",
        "- `DecisionTreeClassifier` from scikit-learn"
      ],
      "metadata": {
        "id": "gX7sJw6Q2zEg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iris = load_iris()\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    iris.data, iris.target, test_size=0.2, random_state=42, stratify=iris.target\n",
        ")\n",
        "\n",
        "custom_tree = CustomDecisionTree(max_depth=3)\n",
        "custom_tree.fit(X_train, y_train)\n",
        "y_pred_custom = custom_tree.predict(X_test)\n",
        "report_classification_results(y_test, y_pred_custom, \"Custom Decision Tree (Iris)\")\n",
        "\n",
        "sk_tree = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "sk_tree.fit(X_train, y_train)\n",
        "y_pred_sklearn = sk_tree.predict(X_test)\n",
        "report_classification_results(y_test, y_pred_sklearn, \"Scikit-learn Decision Tree (Iris)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y99_XQ0B25Pv",
        "outputId": "cb785465-98ef-4de0-890a-b10227ce70df"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom Decision Tree (Iris) -> Accuracy: 0.9667, F1 (macro): 0.9666\n",
            "Scikit-learn Decision Tree (Iris) -> Accuracy: 0.9667, F1 (macro): 0.9666\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9666666666666667, 0.9665831244778612)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Wine Dataset — Decision Tree vs Random Forest\n",
        "\n",
        "This experiment evaluates ensemble learning using Random Forest."
      ],
      "metadata": {
        "id": "UbI1o_a_26H4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wine = load_wine()\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(\n",
        "    wine.data, wine.target, test_size=0.2, random_state=42, stratify=wine.target\n",
        ")\n",
        "\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_tr, y_tr)\n",
        "report_classification_results(y_te, dt.predict(X_te), \"Decision Tree (Wine)\")\n",
        "\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "rf.fit(X_tr, y_tr)\n",
        "report_classification_results(y_te, rf.predict(X_te), \"Random Forest (Wine)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5Cx8xmU26bf",
        "outputId": "e816c81f-c6a8-473c-96ee-ca67fc685c65"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree (Wine) -> Accuracy: 0.9444, F1 (macro): 0.9457\n",
            "Random Forest (Wine) -> Accuracy: 1.0000, F1 (macro): 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.0, 1.0)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Hyperparameter Tuning — GridSearchCV (Random Forest Classifier)\n",
        "\n",
        "Grid search is used to find the best hyperparameters."
      ],
      "metadata": {
        "id": "5ZA9AZMn26rB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'max_features': ['sqrt', 0.5]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(\n",
        "    RandomForestClassifier(random_state=42),\n",
        "    param_grid,\n",
        "    scoring='f1_macro',\n",
        "    cv=5,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "grid.fit(X_tr, y_tr)\n",
        "print(\"Best parameters:\", grid.best_params_)\n",
        "\n",
        "best_rf = grid.best_estimator_\n",
        "report_classification_results(y_te, best_rf.predict(X_te), \"Best Random Forest (Wine)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WHYgpr17264X",
        "outputId": "ce63c546-271d-4e46-8bfd-1831291c13f1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters: {'max_depth': None, 'max_features': 'sqrt', 'n_estimators': 50}\n",
            "Best Random Forest (Wine) -> Accuracy: 1.0000, F1 (macro): 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.0, 1.0)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Regression — Diabetes Dataset\n",
        "\n",
        "We now switch to a **regression problem** and compare:\n",
        "- Decision Tree Regressor\n",
        "- Random Forest Regressor"
      ],
      "metadata": {
        "id": "JVav-MyD27IP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "diabetes = load_diabetes()\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(\n",
        "    diabetes.data, diabetes.target, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "dt_reg = DecisionTreeRegressor(random_state=42)\n",
        "dt_reg.fit(X_tr, y_tr)\n",
        "report_regression_results(y_te, dt_reg.predict(X_te), \"Decision Tree Regressor\")\n",
        "\n",
        "rf_reg = RandomForestRegressor(random_state=42)\n",
        "rf_reg.fit(X_tr, y_tr)\n",
        "report_regression_results(y_te, rf_reg.predict(X_te), \"Random Forest Regressor\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5IUCEnlR27V_",
        "outputId": "21760db0-1cd5-4ea3-fea2-6edb489709a2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Regressor -> RMSE: 70.5464, MSE: 4976.7978\n",
            "Random Forest Regressor -> RMSE: 54.3324, MSE: 2952.0106\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2952.0105887640448, np.float64(54.332408273184846))"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Hyperparameter Tuning — RandomizedSearchCV (Regression)\n",
        "\n",
        "Randomized search is more efficient for large search spaces."
      ],
      "metadata": {
        "id": "M3PGbAau3jTc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import randint\n",
        "\n",
        "param_dist = {\n",
        "    'n_estimators': randint(50, 300),\n",
        "    'max_depth': randint(2, 30),\n",
        "    'min_samples_split': randint(2, 20)\n",
        "}\n",
        "\n",
        "rsearch = RandomizedSearchCV(\n",
        "    RandomForestRegressor(random_state=42),\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=20,\n",
        "    cv=3,\n",
        "    scoring='neg_mean_squared_error',\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "rsearch.fit(X_tr, y_tr)\n",
        "print(\"Best parameters:\", rsearch.best_params_)\n",
        "\n",
        "best_rf_reg = rsearch.best_estimator_\n",
        "report_regression_results(y_te, best_rf_reg.predict(X_te), \"Best Random Forest Regressor\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLClx80x3jmk",
        "outputId": "dbc509a4-513a-47e4-a2ff-b5f64c06b803"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters: {'max_depth': 21, 'min_samples_split': 4, 'n_estimators': 278}\n",
            "Best Random Forest Regressor -> RMSE: 54.8719, MSE: 3010.9214\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3010.9214408853236, np.float64(54.87186383644466))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Conclusion\n",
        "\n",
        "In this worksheet, we:\n",
        "- Built a Decision Tree from scratch\n",
        "- Compared it with scikit-learn implementations\n",
        "- Demonstrated ensemble methods using Random Forests\n",
        "- Applied GridSearchCV and RandomizedSearchCV for hyperparameter tuning"
      ],
      "metadata": {
        "id": "Igy19zzl3j8T"
      }
    }
  ]
}